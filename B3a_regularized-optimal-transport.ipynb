{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Block 3a: Optimal transport with entropic regularization</center>\n",
    "### <center>Alfred Galichon (NYU & Sciences Po)</center>\n",
    "## <center>'math+econ+code' masterclass on optimal transport and economic applications</center>\n",
    "#### <center>With python code examples</center>\n",
    "© 2018-2022 by Alfred Galichon. Past and present support from NSF grant DMS-1716489, ERC grant CoG-866274 are acknowledged, as well as inputs from contributors listed [here](http://www.math-econ-code.org/theteam).\n",
    "\n",
    "**If you reuse material from this masterclass, please cite as:**<br>\n",
    "Alfred Galichon, 'math+econ+code' masterclass on optimal transport and economic applications, January 2022. https://github.com/math-econ-code/mec_optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning objectives\n",
    "\n",
    "* Entropic regularization\n",
    "\n",
    "* The log-sum-exp trick\n",
    "\n",
    "* Gradient descent, coordinate descent\n",
    "\n",
    "* The Iterated Proportional Fitting Procedure (IPFP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* Galichon, *Optimal Transport Methods in Economics*,  Ch. 7.3\n",
    "\n",
    "* Peyré, Cuturi, *Computational Optimal Transport*, Ch. 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropic regularization of the optimal transport problem\n",
    "\n",
    "Consider the problem\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{\\mu\\in\\mathcal{M}\\left(  n,m\\right)  }\\sum_{xy}\\mu_{xy}\\Phi_{xy}-\\sigma\\sum_{xy}\\mu_{xy}\\ln\\mu_{xy}\n",
    "\\end{align*}\n",
    "\n",
    "where $\\sigma>0$. The problem coincides with the optimal assignment problem when $\\sigma=0$. When $\\sigma\\rightarrow+\\infty$, the solution to this problem approaches the independent coupling, $\\mu_{xy}=n_{x}m_{y}$.\n",
    "\n",
    "Later on, we will provide microfoundations for this problem, and connect it with a number of important methods in economics (BLP, gravity model, Choo-Siow...). For now, let's just view this as an extension of the optimal transport problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall compute this problem using Python libraries that we have already met with. Let us start loading them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install -i https://pypi.gurobi.com gurobipy ## only if Gurobi not here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.sparse as spr\n",
    "import gurobipy as grb\n",
    "from sklearn import linear_model\n",
    "from time import time\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also import objects created in the previous lecture, which are stored in the `objects_D1`module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from objects_D1 import OTProblem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the above, Gurobi is for benchmark purposes with the case $\\sigma=0$, but is not suited to compute the nonlinear optimization problem above.\n",
    "\n",
    "---\n",
    "\n",
    "Now, let's load up the `affinitymatrix.csv`, `Xvals.csv` and `Yvals.csv` that you will recall from the previous block. We will work on a smaller population, with `nbx` types of men and `nby` types of women."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thepath = 'https://raw.githubusercontent.com/math-econ-code/mec_optim_2021-01/master/data_mec_optim/marriage_personality-traits/'\n",
    "data_X,data_Y = pd.read_csv(thepath + \"Xvals.csv\"), pd.read_csv(thepath + \"Yvals.csv\")\n",
    "sdX,sdY = data_X.std().values, data_Y.std().values\n",
    "mX,mY = data_X.mean().values, data_Y.mean().values\n",
    "feats_x_k, feats_y_l = ((data_X-mX)/sdX).values, ((data_Y-mY)/sdY).values\n",
    "nbx,nbk = feats_x_k.shape\n",
    "nby,nbl = feats_y_l.shape\n",
    "A_k_l = pd.read_csv(thepath + \"affinitymatrix.csv\").iloc[0:nbk,1:nbl+1].values\n",
    "Φ_x_y = feats_x_k @ A_k_l @ feats_y_l.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract a smaller matrix of size 50x30 to speed up our numerical explorations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbx,nby = 50,30\n",
    "marriage_ex = OTProblem(Φ_x_y[:nbx,:nby],np.ones(nbx) / nbx, np.ones(nby) / nby)\n",
    "nrow , ncol = min(8, nbx) , min(8, nby) # number of rows / cols to displayc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a warm-up, let us compute as in the previous lecture the solution to the problem for $\\sigma=0$ that we can compute with Gurobi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LPsolve(self):\n",
    "    ptm = time()\n",
    "    μ_x_y,u_x,v_y = marriage_ex.solve_full_lp(OutputFlag = False)\n",
    "    taken = time() - ptm \n",
    "    valobs = self.Φ_a.dot(μ_x_y.flatten())\n",
    "    valtot = valobs\n",
    "    ite = None\n",
    "    return μ_x_y,u_x,v_y,valobs,valtot,ite, taken,'LP via Gurobi'\n",
    "\n",
    "OTProblem.LPsolve = LPsolve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will display the output and allow for a benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display( args ):\n",
    "    μ_x_y,u_x,v_y,valobs,valtot,iterations, taken,name = args\n",
    "    print('*'*60)\n",
    "    print('*'* (30 -len(name) // 2) + '  '+name + '  ' + '*'*(26 - (1+len(name)) // 2 ) )\n",
    "    print('*'*60)\n",
    "    print('Converged in ', iterations, ' steps and ', taken, 's.')\n",
    "    print('Sum(mu*Phi)+Sum(mu*log(mu))= ', valtot)\n",
    "    print('Sum(mu*Phi)                = ', valobs)\n",
    "    print('*'*60)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(marriage_ex.LPsolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual of the regularized problem\n",
    "\n",
    "Let's compute the dual by the minimax approach. We have\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{\\mu\\geq0}\\min_{u,v}\\sum_{xy}\\mu_{xy}\\left(  \\Phi_{xy}-u_{x}-v_{y}%\n",
    "-\\sigma\\ln\\mu_{xy}\\right)  +\\sum_{x}u_{x}n_{x}+\\sum_{y}v_{y}m_{y}%\n",
    "\\end{align*}\n",
    "\n",
    "thus\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{u,v}\\sum_{x}u_{x}n_{x}+\\sum_{y}v_{y}m_{y}+\\max_{\\mu\\geq0}\\sum_{xy}%\n",
    "\\mu_{xy}\\left(  \\Phi_{xy}-u_{x}-v_{y}-\\sigma\\ln\\mu_{xy}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "By FOC in the inner problem, one has $\\Phi_{xy}-u_{x}-v_{y}-\\sigma\\ln \\mu_{xy}-\\sigma=0,$thus\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu_{xy}=\\exp\\left(  \\frac{\\Phi_{xy}-u_{x}-v_{y}-\\sigma}{\\sigma}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "and $\\mu_{xy}\\left(  \\Phi_{xy}-u_{x}-v_{y}-\\sigma\\ln\\mu_{xy}\\right) =\\sigma\\mu_{xy}$, thus the dual problem is\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{u,v}\\sum_{x}u_{x}n_{x}+\\sum_{y}v_{y}m_{y}+\\sigma\\sum_{xy}\\exp\\left(\n",
    "\\frac{\\Phi_{xy}-u_{x}-v_{y}-\\sigma}{\\sigma}\\right)  .\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After replacing $v_{y}$ by $v_{y}+\\sigma$, the dual is\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{u,v}\\sum_{x}u_{x}n_{x}+\\sum_{y}v_{y}m_{y}+\\sigma\\sum_{xy}\\exp\\left(\n",
    "\\frac{\\Phi_{xy}-u_{x}-v_{y}}{\\sigma}\\right)  -\\sigma. \\tag{V1}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another expression of the dual\n",
    "\n",
    "**Claim:** the problem is equivalent to\n",
    "\n",
    "<a name='V2'></a>\n",
    "\\begin{align*}\n",
    "\\min_{u,v}\\sum_{x}u_{x}n_{x}+\\sum_{y}v_{y}m_{y}+\\sigma\\log\\sum_{i,j}\n",
    "\\exp\\left(  \\frac{\\Phi_{xy}-u_{x}-v_{y}}{\\sigma}\\right)  \\tag{V2}\n",
    "\\end{align*}\n",
    "\n",
    "Indeed, let us go back to the minimax expression\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{u,v}\\sum_{x}u_{x}n_{x}+\\sum_{y}v_{y}m_{y}+\\max_{\\mu\\geq0}\\sum_{xy}\\mu_{xy}\\left(  \\Phi_{xy}-u_{x}-v_{y}-\\sigma\\ln\\mu_{xy}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "we see that the solution $\\mu$ has automatically $\\sum_{xy}\\mu_{xy}=1$; thus we can incorporate the constraint into\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{u,v}\\sum_{x}u_{x}n_{x}+\\sum_{y}v_{y}m_{y}+\\max_{\\mu\\geq0:\\sum_{xy}\\mu_{xy}=1}\\sum_{xy}\\mu_{xy}\\left(  \\Phi_{xy}-u_{x}-v_{y}-\\sigma\\ln\\mu_{xy}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "which yields the [our desired result](#V2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This expression](#V2) is interesting because, taking *any* $\\hat{\\mu}\\in\n",
    "M\\left(  n,m\\right)$, it reexpresses as\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{u,v}\\sum_{xy}\\hat{\\mu}_{xy}\\left(  \\frac{\\Phi_{xy}-u_{x}-v_{y}}{\\sigma}\\right)  -\\log\\sum_{xy}\\exp\\left(  \\frac{\\Phi_{xy}-u_{x}-v_{y}}{\\sigma}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "therefore if the parameter is $\\theta=\\left(  u,v\\right)$, observations are\n",
    "$xy$ pairs, and the likelihood of $xy$ is\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu_{xy}^{\\theta}=\\frac{\\exp\\left(  \\frac{\\Phi_{xy}-u_{x}-v_{y}}{\\sigma\n",
    "}\\right)  }{\\sum_{xy}\\exp\\left(  \\frac{\\Phi_{xy}-u_{x}-v_{y}}{\\sigma}\\right)\n",
    "}\n",
    "\\end{align*}\n",
    "\n",
    "Hence, [our expression](#problem) will coincide with the maximum likelihood in this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A third expression of the dual problem\n",
    "\n",
    "Consider\n",
    "\n",
    "<a name='V2'></a>\n",
    "\\begin{align*}\n",
    "\\min_{u,v}  &  \\sum_{x}u_{x}n_{x}+\\sum_{y}v_{y}m_{y} \\\\\n",
    "s.t. \\quad &  \\sum_{i,j}\\exp\\left(  \\frac{\\Phi_{xy}-u_{x}-v_{y}}{\\sigma}\\right)\n",
    "=1\n",
    "\\end{align*}\n",
    "\n",
    "It is easy to see that the solutions of this problem coincide with [version 2](#V2). Indeed, the Lagrange multiplier is forced to be one. In other words,\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{u,v}  &  \\sum_{x}u_{x}n_{x}+\\sum_{y}v_{y}m_{y}\\\\\n",
    "s.t. \\quad &  \\sigma\\log\\sum_{i,j}\\exp\\left(  \\frac{\\Phi_{xy}-u_{x}-v_{y}}{\\sigma\n",
    "}\\right)  =0\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small-temperature limit and the log-sum-exp trick\n",
    "\n",
    "Recall that when $\\sigma\\rightarrow0$, one has\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma\\log\\left(  e^{a/\\sigma}+e^{b/\\sigma}\\right)  \\rightarrow\\max\\left(\n",
    "a,b\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Indeed, letting $m=\\max\\left(  a,b\\right)$,\n",
    "\n",
    "<a name='lse'></a>\n",
    "\\begin{align*}\n",
    "\\sigma\\log\\left(  e^{a/\\sigma}+e^{b/\\sigma}\\right)  =m+\\sigma\\log\\left(\\exp\\left(  \\frac{a-m}{\\sigma}\\right)  +\\exp\\left(  \\frac{b-m}{\\sigma}\\right)\\right),\n",
    "\\end{align*}\n",
    "and the argument of the logarithm lies between $1$ and $2$.\n",
    "\n",
    "This simple remark is actually a useful numerical recipe called the *log-sum-exp trick*: when $\\sigma$ is small, using [the formula above](#lse) to compute $\\sigma\\log\\left(  e^{a/\\sigma}+e^{b/\\sigma}\\right)$ ensures the exponentials won't blow up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The log-sum-exp trick for regularized OT\n",
    "\n",
    "Back to the third expression, with $\\sigma\\rightarrow0$, one has\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{u,v}  &  \\sum_{x}u_{x}n_{x}+\\sum_{y}v_{y}m_{y}\\tag{V3}\\\\\n",
    "s.t.  &  \\max_{xy}\\left(  \\Phi_{xy}-u_{x}-v_{y}\\right)  =0\\nonumber\n",
    "\\end{align*}\n",
    "\n",
    "This is exactly equivalent with the classical Monge-Kantorovich expression\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{u,v}  &  \\sum_{x}u_{x}n_{x}+\\sum_{y}v_{y}m_{y}\\tag{V3}\\\\\n",
    "s.t.  &  \\Phi_{xy}-u_{x}-v_{y}\\leq0\\nonumber\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the third expression of the dual, with $\\sigma\\rightarrow0$, one has\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{u,v}  &  \\sum_{x}u_{x}n_{x}+\\sum_{y}v_{y}m_{y}\\tag{V3}\\\\\n",
    "s.t.  &  \\max_{xy}\\left(  \\Phi_{xy}-u_{x}-v_{y}\\right)  =0\\nonumber\n",
    "\\end{align*}\n",
    "\n",
    "This is exactly equivalent with the classical Monge-Kantorovich expression\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{u,v}  &  \\sum_{x}u_{x}n_{x}+\\sum_{y}v_{y}m_{y}\\tag{V3}\\\\\n",
    "s.t.  &  \\Phi_{xy}-u_{x}-v_{y}\\leq0\\nonumber\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation\n",
    "\n",
    "We can compute $\\min F\\left(  x\\right)$ by two methods:\n",
    "\n",
    "Either by gradient descent: $x\\left(  t+1\\right)  =x_{t}-\\epsilon _{t}\\nabla F\\left(  x_{t}\\right)  $. (Steepest descent has $\\epsilon _{t}=1/\\left\\vert \\nabla F\\left(  x_{t}\\right)  \\right\\vert $.)\n",
    "\n",
    "Or by coordinate descent: $x_{k}\\left(  t+1\\right)  =\\arg\\min_{x_{k}}F\\left(  x_{k},x_{-k}\\left(  t\\right)  \\right)$.\n",
    "\n",
    "Why do these methods converge? Let's provide some justification. We will decrease $x_{t}$ by $\\epsilon d_{t}$, were $d_{t}$ is normalized by $\\left\\vert d_{t}\\right\\vert _{p}:=\\left(  \\sum_{i=1}^{n}d_{t}^{i}\\right) ^{1/p}=1$. At first order, we have \n",
    "\n",
    "\\begin{align*}\n",
    "F\\left(  x(t)-\\epsilon d_{t}\\right)  =F\\left(  x(t)\\right)  -\\epsilon d_{t}^{\\intercal}\\nabla F\\left(  x (t)\\right)  +O\\left(  \\epsilon^{1}\\right).\n",
    "\\end{align*}\n",
    "\n",
    "We need to maximize $d_{t}^{\\intercal}\\nabla F\\left(  x(t)\\right)$ over $\\left\\vert d_{t}\\right\\vert _{p}=1$.\n",
    "\n",
    "* For $p=2$, we get $d_{t}=\\nabla F\\left(  x(t)\\right)  /\\left\\vert \\nabla F\\left(  x(t)\\right)  \\right\\vert $\n",
    "\n",
    "* For $p=1$, we get $d_{t}=sign\\left(  \\partial F\\left(  x(t)\\right)/\\partial x_{k}\\right)  $ if $\\left\\vert \\partial F\\left(  x_{t}\\right) /\\partial x_{k}\\right\\vert =\\max_{l}\\left\\vert \\partial F\\left(  x(t)\\right) /\\partial x_{l}\\right\\vert $, $0$ otherwise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our context, gradient descent is\n",
    "\n",
    "\\begin{align*}\n",
    "u_{x}\\left(  t+1\\right)    & =u_{x}\\left(  t\\right)  -\\epsilon\\frac{\\partial\n",
    "F}{\\partial u_{x}}\\left(  u\\left(  t\\right)  ,v\\left(  t\\right)  \\right)\n",
    ",\\text{ and }\\\\\n",
    "v_{y}\\left(  t+1\\right)    & =v_{y}\\left(  t\\right)  -\\epsilon\\frac{\\partial\n",
    "F}{\\partial v_{y}}\\left(  u\\left(  t\\right)  ,v\\left(  t\\right)  \\right)\n",
    "\\end{align*}\n",
    "\n",
    "while coordinate descent is\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial F}{\\partial u_{x}}\\left(  u_{x}\\left(  t+1\\right)\n",
    ",u_{-i}\\left(  t\\right)  ,v\\left(  t\\right)  \\right)  =0,\\text{ and }\n",
    "\\frac{\\partial F}{\\partial v_{y}}\\left(  u\\left(  t\\right)  ,v_{y}\\left(\n",
    "t+1\\right)  ,v_{-j}\\left(  t\\right)  \\right)  =0.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "Gradient of objective function in version 1 of our problem:\n",
    "\n",
    "\\begin{align*}\n",
    "\\left(  n_{x}-\\sum_{y}\\exp\\left(  \\frac{\\Phi_{xy}-u_{x}-v_{y}}{\\sigma}\\right)\n",
    ",m_{y}-\\sum_{x}\\exp\\left(  \\frac{\\Phi_{xy}-u_{x}-v_{y}}{\\sigma}\\right)\n",
    "\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Gradient of objective function in version 2\n",
    "\n",
    "\\begin{align*}\n",
    "\\left(  n_{x}-\\frac{\\sum_{y}\\exp\\left(  \\frac{\\Phi_{xy}-u_{x}-v_{y}}{\\sigma\n",
    "}\\right)  }{\\sum_{xy}\\exp\\left(  \\frac{\\Phi_{xy}-u_{x}-v_{y}}{\\sigma}\\right)\n",
    "},m_{y}-\\frac{\\sum_{x}\\exp\\left(  \\frac{\\Phi_{xy}-u_{x}-v_{y}}{\\sigma}\\right)\n",
    "}{\\sum_{xy}\\exp\\left(  \\frac{\\Phi_{xy}-u_{x}-v_{y}}{\\sigma}\\right)  }\\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate descent\n",
    "\n",
    "Coordinate descent on objective function in version 1:\n",
    "\n",
    "\\begin{align*}\n",
    "n_{x}  & =\\sum_{y}\\exp\\left(  \\frac{\\Phi_{xy}-u_{x}\\left(  t+1\\right)\n",
    "-v_{y}\\left(  t\\right)  }{\\sigma}\\right)  ,\\\\\n",
    "m_{y}  & =\\sum_{x}\\exp\\left(  \\frac{\\Phi_{xy}-u_{x}\\left(  t\\right)\n",
    "-v_{y}\\left(  t+1\\right)  }{\\sigma}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "that is\n",
    "\n",
    "\\begin{align*}\n",
    "\\left\\{\n",
    "\\begin{array}\n",
    "[c]{c}\n",
    "u_{x}\\left(  t+1\\right)  =\\sigma\\log\\left(  \\frac{1}{n_{x}}\\sum_{y}\\exp\\left(\n",
    "\\frac{\\Phi_{xy}-v_{y}\\left(  t\\right)  }{\\sigma}\\right)  \\right)  \\\\\n",
    "v_{y}\\left(  t+1\\right)  =\\sigma\\log\\left(  \\frac{1}{m_{y}}\\sum_{x}\\exp\\left(\n",
    "\\frac{\\Phi_{xy}-u_{x}\\left(  t\\right)  }{\\sigma}\\right)  \\right)\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{align*}\n",
    "\n",
    "this is called the Iterated Fitting Proportional Procedure (IPFP), or Sinkhorn's algorithm.\n",
    "\n",
    "Coordinate descent on objective function in version 2 does not yield a closed-form expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IPFP, matrix version\n",
    "\n",
    "Letting $a_{x}=\\exp\\left(  -u_{x}/\\sigma\\right)  $ and $b_{y}=\\exp\\left(  -v_{y}/\\sigma\\right)  $ and $K_{xy}=\\exp\\left(  \\Phi_{xy}/\\sigma\\right)  $, one has $\\mu_{xy}=a_{x}b_{y}K_{xy}$, and the procedure reexpresses as\n",
    "\n",
    "\\begin{align*}\n",
    "\\left\\{\n",
    "\\begin{array}\n",
    "[c]{l}%\n",
    "a_{x}\\left(  t+1\\right)  =n_{x}/\\left(  Kb\\left(  t\\right)  \\right)\n",
    "_{x}\\text{ and }\\\\\n",
    "b_{y}\\left(  t+1\\right)  =m_{y}/\\left(  K^{\\intercal}a\\left(  t\\right)\n",
    "\\right)  _{y}.\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{align*}\n",
    "\n",
    "Because this algorithm involves matrix operations only, and is naturally suited for parallel computation, GPUs are a tool of choice for addressing is. See chap. 4 of Peyré and Cuturi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation \n",
    "First, as a convenience, we would like to have a function that computes $\\sum_k \\mu_k \\log \\mu_k$ without failing when some of the entries of $\\mu_k$ are zero. We code this into:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_xlogx(a):\n",
    "    s=a.sum()\n",
    "    return s*np.log(s) - s * entropy(a.flatten(),axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement this algorithm. Return to the matrix-IPFP algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrixIPFP(self,σ , tol = 1e-9, maxite = 1e+06 ):\n",
    "    ptm = time()\n",
    "    ite = 0\n",
    "    K_x_y = np.exp(self.Φ_a / σ).reshape(self.nbx,-1)\n",
    "    B_y = np.ones(self.nby)\n",
    "    error = tol + 1\n",
    "    while error > tol and ite < maxite:\n",
    "        A_x = self.n_x / (K_x_y @ B_y)\n",
    "        KA_y = (A_x.T @ K_x_y)\n",
    "        error = (abs(KA_y * B_y / self.m_y)-1).max()\n",
    "        B_y = self.m_y / KA_y\n",
    "        ite = ite + 1\n",
    "        \n",
    "    u_x,v_y = - σ * np.log(A_x),- σ * np.log(B_y)\n",
    "    μ_x_y = K_x_y * A_x.reshape((-1,1)) * B_y.reshape((1,-1))\n",
    "    valobs = self.Φ_a.dot(μ_x_y.flatten())\n",
    "    valtot =  valobs - σ * sum_xlogx(μ_x_y)\n",
    "    taken = time() - ptm\n",
    "    if ite >= maxite:\n",
    "        print('Maximum number of iteations reached in matrix IPFP.')    \n",
    "    return μ_x_y,u_x,v_y,valobs,valtot,ite, taken, 'matrix IPFP'\n",
    "\n",
    "OTProblem.matrixIPFP = matrixIPFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(marriage_ex.matrixIPFP(0.1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the benefit of the matrix version, let us recode the same algorithm as above, but in the log-domain, namely iterate over the values of $u$ and $v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logdomainIPFP(self,σ = 0.1, tol = 1e-9, maxite = 1e+06 ):\n",
    "    ptm = time()\n",
    "    ite = 0\n",
    "    Φ_x_y = self.Φ_a.reshape(self.nbx,-1)\n",
    "    v_y = np.zeros(nby)\n",
    "    λ_x,ζ_y = - σ * np.log(self.n_x), - σ * np.log(self.m_y)\n",
    "    error = tol + 1\n",
    "    while error > tol and ite < maxite:\n",
    "        u_x = λ_x + σ * np.log( (np.exp((Φ_x_y - v_y.reshape((1,-1)))/σ)).sum( axis=1) )\n",
    "        KA_y = (np.exp((Φ_x_y -u_x.reshape((-1,1))) / σ)).sum(axis=0)\n",
    "        error = np.max(np.abs(KA_y * np.exp(-v_y / σ) / self.m_y - 1))\n",
    "        v_y = ζ_y + σ * np.log(KA_y)\n",
    "        ite = ite + 1\n",
    "    \n",
    "    μ_x_y =np.exp((Φ_x_y -u_x.reshape((-1,1)) - v_y.reshape((1,-1)))/σ )\n",
    "    valobs = self.Φ_a.dot(μ_x_y.flatten())\n",
    "    valtot =  valobs - σ * sum_xlogx(μ_x_y)\n",
    "    taken = time() - ptm\n",
    "    if ite >= maxite:\n",
    "        print('Maximum number of iteations reached in log-domain IPFP.')\n",
    "    return μ_x_y,u_x,v_y,valobs,valtot,ite, taken, 'log-domain IPFP'\n",
    "\n",
    "OTProblem.logdomainIPFP = logdomainIPFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(marriage_ex.logdomainIPFP(0.1))\n",
    "display(marriage_ex.matrixIPFP(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the log-domain IPFP, while  mathematically equivalent to matrix IPFP, it is noticeably slower. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IPFP with the log-sum-exp trick\n",
    "\n",
    "The matrix IPFPis very fast, partly due to the fact that it involves linear algebra operations. However, it breaks down when $\\sigma$ is small; this is best seen taking a log transform and returning to $u^{k}=-\\sigma\\log a^{k}$ and $v^{k}=-\\sigma\\log b^{k}$, that is\n",
    "\n",
    "\\begin{align*}\n",
    "\\left\\{\n",
    "\\begin{array}\n",
    "[c]{l}%\n",
    "u_{x}^{k}=\\mu_{x}+\\sigma\\log\\sum_{y}\\exp\\left(  \\frac{\\Phi_{xy}-v_{y}^{k-1}%\n",
    "}{\\sigma}\\right) \\\\\n",
    "v_{y}^{k}=\\zeta_{y}+\\sigma\\log\\sum_{x}\\exp\\left(  \\frac{\\Phi_{xy}-u_{x}^{k}%\n",
    "}{\\sigma}\\right)\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mu_{x}=-\\sigma\\log n_{x}$ and $\\zeta_{y}=-\\sigma\\log m_{y}$.\n",
    "\n",
    "One sees what may go wrong: if $\\Phi_{xy}-v_{y}^{k-1}$ is positive in the exponential in the first sum, then the exponential blows up due to the small $\\sigma$ at the denominator. However, the log-sum-exp trick can be used in order to avoid this issue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider\n",
    "\n",
    "\\begin{align*}\n",
    "\\left\\{\n",
    "\\begin{array}\n",
    "[c]{l}%\n",
    "\\tilde{v}_{x}^{k}=\\max_{y}\\left\\{  \\Phi_{xy}-v_{y}^{k}\\right\\} \\\\\n",
    "\\tilde{u}_{y}^{k}=\\max_{x}\\left\\{  \\Phi_{xy}-u_{x}^{k}\\right\\}\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{align*}\n",
    "\n",
    "(the indexing is not a typo: $\\tilde{v}$ is indexed by $i$ and $\\tilde{u}$ by $j$).\n",
    "\n",
    "One has\n",
    "\n",
    "\\begin{align*}\n",
    "\\left\\{\n",
    "\\begin{array}\n",
    "[c]{l}%\n",
    "u_{x}^{k}=\\mu_{x}+\\tilde{v}_{x}^{k-1}+\\sigma\\log\\sum_{y}\\exp\\left(  \\frac\n",
    "{\\Phi_{xy}-v_{y}^{k-1}-\\tilde{v}_{x}^{k}}{\\sigma}\\right) \\\\\n",
    "v_{y}^{k}=\\zeta_{y}+\\tilde{u}_{y}^{k}+\\sigma\\log\\sum_{x}\\exp\\left(  \\frac\n",
    "{\\Phi_{xy}-u_{x}^{k}-\\tilde{u}_{y}^{k}}{\\sigma}\\right)\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{align*}\n",
    "\n",
    "and now the arguments of the exponentials are always nonpositive, ensuring the exponentials don't blow up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the matrix version and the log-domain version of the IPFP  will break down when $\\sigma$ is small, e.g. $\\sigma=0.001$ (Try!). However if we modify the second procedure using the log-sum-exp trick, things work again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logdomainIPFP_with_LSE_trick(self,σ , tol = 1e-9, maxite = 1e+06 ):\n",
    "    ptm = time()\n",
    "    ite = 0\n",
    "    Φ_x_y = self.Φ_a.reshape(self.nbx,-1)\n",
    "    v_y = np.zeros(nby)\n",
    "    λ_x,ζ_y = - σ * np.log(self.n_x), - σ * np.log(self.m_y)\n",
    "    error = tol + 1\n",
    "    while error > tol and ite < maxite:\n",
    "        vstar_x = (Φ_x_y - v_y.reshape((1,-1))).max( axis = 1)\n",
    "        u_x = λ_x + vstar_x + σ * np.log( (np.exp((Φ_x_y - vstar_x.reshape((-1,1)) - v_y.reshape((1,-1)))/σ)).sum( axis=1) )\n",
    "        ustar_y = (Φ_x_y - u_x.reshape((-1,1)) ).max( axis = 0)\n",
    "        KA_y = (np.exp((Φ_x_y -u_x.reshape((-1,1)) - ustar_y.reshape((1,-1)) ) / σ)).sum(axis=0)\n",
    "        error = np.max(np.abs(KA_y * np.exp( (ustar_y-v_y) / σ) / self.m_y - 1))\n",
    "        v_y = ζ_y + ustar_y+ σ * np.log(KA_y)\n",
    "        ite = ite + 1\n",
    "    μ_x_y =np.exp((Φ_x_y -u_x.reshape((-1,1)) - v_y.reshape((1,-1)))/σ )\n",
    "    valobs = self.Φ_a.dot(μ_x_y.flatten())\n",
    "    valtot =  valobs - σ * sum_xlogx(μ_x_y)\n",
    "    taken = time() - ptm\n",
    "    if ite >= maxite:\n",
    "        print('Maximum number of iteations reached in log-domain IPFP with LSE trick.')\n",
    "    return μ_x_y,u_x,v_y,valobs,valtot,ite, taken, 'log-domain IPFP with LSE trick'\n",
    "\n",
    "OTProblem.logdomainIPFP_with_LSE_trick = logdomainIPFP_with_LSE_trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(marriage_ex.logdomainIPFP_with_LSE_trick(0.1))\n",
    "display(marriage_ex.logdomainIPFP(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, when $\\sigma = 0.01$ we see that the algorithm works with the log-sum-exp trick but fails without:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(marriage_ex.logdomainIPFP_with_LSE_trick(0.1))\n",
    "# display(marriage_ex.logdomainIPFP(0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computations using GLM\n",
    "\n",
    "Recall that the *margining matrix* of shape $\\left( nbx+nby\\right) \\times \\left(\n",
    "nbx.nby\\right) $ is given by\n",
    "\n",
    "$M=\\binom{I_{X}\\otimes 1_{Y}^{\\top }}{1_{Y}^{\\top }\\otimes I_{Y}}$\n",
    "\n",
    "Introduce $\\hat{\\mu}=nm^{\\top } / (\\sum_x n_x)$\n",
    "\n",
    "We have $M\\hat{\\mu}=\\binom{n}{m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem writes\n",
    "\n",
    "$\\min_{p}1_{\\mathcal{A}}^{\\top }\\exp \\left( \\frac{\\Phi -M^{\\top }p}{\\sigma }%\n",
    "\\right) -\\hat{\\mu}^{\\top }\\left( \\frac{\\Phi -M^{\\top }p}{\\sigma }\\right) $\n",
    "\n",
    "And setting $\\tilde{p}=p/\\sigma $ and $\\tilde{\\Phi}=\\Phi /\\sigma $ yields\n",
    "that $\\tilde{p}$ is obtained by\n",
    "\n",
    "$\\min_{\\tilde{p}}1^{\\top }\\exp \\left( \\tilde{\\Phi}-M^{\\top }\\tilde{p}\\right)\n",
    "-\\hat{\\mu}^{\\top }\\left( \\tilde{\\Phi}-M^{\\top }\\tilde{p}\\right) $\n",
    "\n",
    "which we can rewrite as a weighted Poisson regression \n",
    "\n",
    "$\\min_{\\tilde{p}}\\sum_{a\\in \\mathcal{A}}w_{a}\\exp \\left( -\\left( M^{\\top }%\n",
    "\\tilde{p}\\right) _{a}\\right) -\\sum_{a\\in \\mathcal{A}}w_{a}\\hat{\\mu}_{a}e^{-%\n",
    "\\tilde{\\Phi}_{a}}\\left( \\tilde{\\Phi}-M^{\\top }\\tilde{p}\\right) _{a}$\n",
    "\n",
    "where $w_{a}=\\exp \\tilde{\\Phi}_{a}$\n",
    "\n",
    "Dropping the constant term, this implements \n",
    "\n",
    "$\\min_{\\tilde{p}}\\sum_{a\\in \\mathcal{A}}w_{a}\\exp \\left( -M^{\\top }\\tilde{p}%\n",
    "\\right) _{a}+\\sum_{a\\in \\mathcal{A}}w_{a}\\left( \\hat{\\mu}e^{-\\tilde{\\Phi}%\n",
    "}\\right) _{a}\\left( M^{\\top }\\tilde{p}\\right) _{a}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solveGLM(self,σ , tol = 1e-9):\n",
    "    ptm = time()\n",
    "    muhat_a = (self.n_x.reshape((nbx,-1)) @ self.m_y.reshape((-1,nby))).flatten() / self.n_x.sum()\n",
    "    ot_as_glm = linear_model.PoissonRegressor(fit_intercept=False,tol=tol ,verbose=3,alpha=0)\n",
    "    ot_as_glm.fit( - self.M_z_a().T, muhat_a * np.exp(-self.Φ_a / σ) , sample_weight = np.exp(self.Φ_a / σ))\n",
    "    \n",
    "    p = σ * ot_as_glm.coef_\n",
    "    u_x,v_y  = p[:nbx] - p[0], p[nbx:]+p[0]\n",
    "    μ_x_y =np.exp((self.Φ_a.reshape((self.nbx,-1)) -u_x.reshape((-1,1)) - v_y.reshape((1,-1)))/σ )\n",
    "    valobs = self.Φ_a.dot(μ_x_y.flatten())\n",
    "    valtot =  valobs - σ * sum_xlogx(μ_x_y)\n",
    "    taken = time() - ptm\n",
    "    return μ_x_y,u_x,v_y,valobs,valtot,None, taken, 'GLM'\n",
    "\n",
    "OTProblem.solveGLM = solveGLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "σ = 0.5\n",
    "display(marriage_ex.solveGLM(σ ))\n",
    "display(marriage_ex.matrixIPFP(σ ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when $\\sigma = 0.1$ the GLM approach fails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "σ = 0.1\n",
    "display(marriage_ex.solveGLM(σ ))\n",
    "display(marriage_ex.matrixIPFP(σ ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
