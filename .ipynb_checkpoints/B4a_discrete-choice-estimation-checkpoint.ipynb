{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Block 4a Discrete choice estimation</center>\n",
    "### <center>Alfred Galichon (NYU & Sciences Po)</center>\n",
    "## <center>'math+econ+code' masterclass on optimal transport and economic applications</center>\n",
    "#### <center>With python code examples</center>\n",
    "© 2018-2022 by Alfred Galichon. Past and present support from NSF grant DMS-1716489, ERC grant CoG-866274 are acknowledged, as well as inputs from contributors listed [here](http://www.math-econ-code.org/theteam).\n",
    "\n",
    "**If you reuse material from this masterclass, please cite as:**<br>\n",
    "Alfred Galichon, 'math+econ+code' masterclass on optimal transport and economic applications, January 2022. https://github.com/math-econ-code/mec_optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* Savage, L. (1951). The theory of statistical decision. JASA.\n",
    "* Bonnet, Fougère, Galichon, Poulhès (2021). Minimax estimation of hedonic models. Preprint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the libraries\n",
    "\n",
    "First, let's load the libraries we shall need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import string as str\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import scipy.sparse as spr\n",
    "from scipy import optimize, special\n",
    "# !python -m pip install -i https://pypi.gurobi.com gurobipy ## only if Gurobi not here\n",
    "import gurobipy as grb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our data\n",
    "We will go back to the dataset of Greene and Hensher (1997). As a reminder, 210 individuals are surveyed about their choice of travel mode between Sydney, Canberra and Melbourne, and the various costs (time and money) associated with each alternative. Therefore there are 840 = 4 x 210 observations, which we can stack into `travelmodedataset` a 3 dimensional array whose dimensions are mode,individual,dummy for choice+covariates.\n",
    "\n",
    "Let's load the dataset and represent it conveniently in a similar fashion as in block 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thepath = os.path.join(os.getcwd(),'data_mec_optim/demand_travelmode/')\n",
    "thepath = 'https://raw.githubusercontent.com/math-econ-code/mec_optim_2021-01/master/data_mec_optim/demand_travelmode/'\n",
    "\n",
    "travelmode =  pd.read_csv(thepath+'travelmodedata.csv')\n",
    "\n",
    "travelmode['choice'] = np.where(travelmode['choice'] =='yes' , 1, 0)\n",
    "\n",
    "nobs = travelmode.shape[0]\n",
    "ncols = travelmode.shape[1]\n",
    "nbchoices = 4\n",
    "ninds = int(nobs/nbchoices)\n",
    "\n",
    "muhat_i_y = travelmode['choice'].values.reshape(ninds,nbchoices).T\n",
    "muhat_iy = muhat_i_y.flatten()\n",
    "\n",
    "muhat_i_y = travelmode['choice'].values.reshape(ninds,4).T\n",
    "muhat_iy = muhat_i_y.flatten()\n",
    "\n",
    "s_y = travelmode.groupby(['mode']).mean()['choice'].to_frame().sort_index()\n",
    "\n",
    "def two_d(X):\n",
    "    return np.reshape(X,(X.size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation with no observable heterogeneity\n",
    "\n",
    "Start with assuming that there is no observable heterogeneity, so the only observation we have at hand are the aggregate market shares $s_y$. Hence the systematic utility will be the same for every agent. However, we wish to write a parametric model for it, namely assume a knwon parametric form for the dependence of $U_y$ with respect to various observed characteristics associated with $y$.\n",
    "\n",
    "Assume then that the utilities are parameterized as follows: $U = \\Phi \\beta$ where $\\beta\\in\\mathbb{R}^{p}$ is a parameter, and $\\Phi$ is a $\\left\\vert \\mathcal{Y}\\right\\vert \\times p$ matrix.\n",
    "\n",
    "The log-likelihood function is given by\n",
    "\n",
    "\\begin{align*}\n",
    "l\\left(  \\beta\\right)  =N\\sum_{y}\\hat{s}_{y}\\log\\sigma_{y}\\left(\\Phi \\beta\\right)\n",
    "\\end{align*}\n",
    "\n",
    "A common estimation method of $\\beta$ is by maximum likelihood%\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{\\beta}l\\left(  \\beta\\right)  .\n",
    "\\end{align*}\n",
    "\n",
    "MLE is statistically efficient; the problem is that the problem is not guaranteed to be convex, so there may be computational difficulties (e.g. local optima)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE, logit case\n",
    "\n",
    "In the logit case,\n",
    "\n",
    "\\begin{align*}\n",
    "l\\left(  \\beta\\right)  =N\\left\\{  \\hat{s}^{\\intercal}\\Phi\\beta-\\log\\sum_{y}\\exp\\left(  \\Phi\\beta\\right)  _{y}\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "so that the max-likehood amounts to\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{\\beta}\\left\\{  \\hat{s}^{\\intercal} \\Phi \\beta-G\\left( \\Phi \\beta\\right)\n",
    "_{y}\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "whose value is the Legendre-Fenchel transform of $\\beta\\rightarrow G\\left( \\Phi \\beta\\right)$ evaluated at $\\Phi ^{^{\\intercal}}\\hat{s}$.\n",
    "\n",
    "Note that the vector $\\Phi^{^{\\intercal}}\\hat{s}$ is the vector of empirical moments, which is a sufficient statistics in the logit model.\n",
    "\n",
    "As a result, in the logit case, the MLE is a convex optimization problem, and it is therefore both statistically efficient and computationally efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moment estimation\n",
    "\n",
    "The previous remark will inspire an alternative procedure based on the moments statistics $\\Phi^{^{\\intercal}}\\hat{s}$.\n",
    "\n",
    "The social welfare is given in general by $W\\left(  \\beta\\right) =G\\left(  \\Phi\\beta\\right)  $. One has $\\partial_{\\beta^{i}}W\\left(\\beta\\right)  =\\sum_{y}\\sigma_{y}\\left(  \\Phi\\beta\\right)  \\Phi_{yi}$, that is \n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla W\\left(  \\beta\\right)  = \\Phi^{\\intercal}\\sigma\\left(  \\Phi\\beta\\right)  ,\n",
    "\\end{align*}\n",
    "\n",
    "which is the vector of predicted moments.\n",
    "\n",
    "Therefore the program\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{\\beta}\\left\\{  \\hat{s}^{\\intercal}\\Phi\\beta-G\\left(  \\Phi\\beta\\right)\n",
    "_{y}\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "picks up the parameter $\\beta$ which matches the empirical moments $X^{^{\\intercal}}\\hat{s}$ with the predicted ones $\\nabla W\\left(\\beta\\right)  $. This procedure is not statistically efficient, but is computationally efficient becauses it arises from a convex optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed temperature MLE\n",
    "\n",
    "Back to the logit case. Recall we have\n",
    "\n",
    "\\begin{align*}\n",
    "l\\left(  \\beta\\right)  =N\\left\\{  \\hat{s}^{\\intercal}\\Phi\\beta-\\log\\sum_{y} \\exp\\left(  \\Phi\\beta\\right)  _{y}\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "Assume that we restrict ourselves to $\\beta^{\\top}z>0$. Then we can write $\\beta=\\theta/T$ where $T=1/\\beta^{\\top}z$ and $\\theta=\\beta T$. Call $\\Theta=\\left\\{  \\theta\\in\\mathbb{R}^{p},\\theta^{\\top}z=1\\right\\}  $, so that $\\beta=\\theta/T$ where $\\theta\\in\\Theta$ and $T>0$. We have\n",
    "\n",
    "\\begin{align*}\n",
    "l\\left(  \\theta,T\\right)  =\\frac{N}{T}\\left\\{  \\hat{s}^{\\intercal}\n",
    "\\Phi\\theta-T\\log\\sum_{y}\\exp\\left(  \\frac{\\left(  \\Phi\\theta\\right)  _{y}}{T}\\right)  \\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "and we define the *fixed temperature maximum likelihood estimator* by\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta\\left(  T\\right)  =\\arg\\max_{\\theta}l\\left(  \\theta,T\\right)\n",
    "\\end{align*}\n",
    "\n",
    " Note that $\\theta\\left(  T\\right)  =\\arg\\max_{\\theta\\in\\Theta}Tl\\left(\\theta,T\\right)$ where\n",
    "\n",
    "\\begin{align*}\n",
    "Tl\\left(  \\theta,T\\right)  =N\\left\\{  \\hat{s}^{\\intercal}\\Phi\\theta-T\\log\\sum _{y}\\exp\\left(  \\frac{\\left(  \\Phi\\theta\\right)  _{y}}{T}\\right)  \\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "and we note that $Tl\\left(  \\theta,T\\right)  \\rightarrow N\\left\\{  \\hat{s}^{\\intercal}\\Phi\\theta-\\max_{y\\in\\mathcal{Y}}\\left\\{  \\left(  \\Phi\\theta\\right)_{y}\\right\\}  \\right\\}  $ as $T\\rightarrow0$.\n",
    "\n",
    "We have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{Tl\\left(  \\theta,T\\right)  }{N}=\\hat{s}^{\\intercal}\\Phi\\theta-T\\log\\sum_{y}\\exp\\left(  \\frac{\\left(  \\Phi\\theta\\right)  _{y}}{T}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Let $\\theta\\left(  0\\right)  =\\lim_{T\\rightarrow0}\\theta\\left(T\\right)  $. Calling $m\\left(  \\theta\\right)  =\\max_{y\\in\\mathcal{Y}}\\left\\{\\left(  \\Phi\\theta\\right)  _{y}\\right\\}  $, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta\\left(  0\\right)  \\in\\arg\\max_{\\theta}\\left\\{  \\hat{s}^{\\intercal}\\Phi\\theta-m\\left(  \\theta\\right)  \\right\\},\n",
    "\\end{align*}\n",
    "\n",
    "or\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta\\left(  0\\right)  \\in\\arg\\min_{\\theta}\\left\\{  m\\left(  \\theta\\right)-\\hat{s}^{\\intercal}\\Phi\\theta\\right\\},\n",
    "\\end{align*}\n",
    "\n",
    "Calling $m\\left(  \\theta\\right)  =\\max_{y\\in\\mathcal{Y}}\\left\\{  \\left(\\Phi\\theta\\right)  _{y}\\right\\}  $, one has \n",
    "\n",
    "\\begin{align*}\n",
    "\\theta\\left(  T\\right)  \\in\\arg\\max\\left\\{  \\hat{s}^{\\intercal}\\Phi\\theta-m\\left(  \\theta\\right)  -T\\log\\sum_{y}\\exp\\left(  \\frac{\\left(\\Phi\\theta\\right)  _{y}-m\\left(  \\theta\\right)  }{T}\\right)  \\right\\}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimax-regret estimation\n",
    "\n",
    "Note that\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta\\left(  0\\right)  \\in\\arg\\max\\left\\{  \\hat{s}^{\\intercal}\\Phi\\theta\n",
    "-m\\left(  \\theta\\right)  \\right\\}  .\n",
    "\\end{align*}\n",
    "\n",
    "Define $R_{i}\\left(  \\theta,y\\right)  =\\left(  \\Phi\\theta\\right)_{y}-\\left(  \\Phi\\theta\\right)  _{y_{i}}$ the regret associated with observation $i$ with respect to $y$. This is equal to the difference between the payoff given by $y$ and the payoff obtained under observation $i$, denoting $y_{i}$ the action taken in observation $i$. The max-regret associated with observation $i$ is therefore\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{y\\in\\mathcal{Y}}R_{i}\\left(  \\theta,y\\right)  =\\max_{y\\in\\mathcal{Y}}\\left\\{  \\left(  \\Phi\\theta\\right)_{y}-\\left(  \\Phi\\theta\\right)_{y_{i}}\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "and the max-regret associated with the sample is $\\frac{1}{N}\\sum\\max_{y\\in\\mathcal{Y}}\\left\\{  R_{i}\\left(  \\theta,y\\right)  \\right\\}  $, that is $\\max_{y\\in\\mathcal{Y}}\\left\\{  \\left(  \\Phi\\theta\\right)  _{y}\\right\\} - \\hat{s}^{\\intercal}X\\theta$.\n",
    "\n",
    "The minimax regret estimator\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\theta}^{MMR}=\\min_{\\theta}\\left\\{  m\\left(  \\theta\\right)  -\\hat\n",
    "{s}^{\\intercal}\\Phi\\theta\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "which has a linear programming fomulation\n",
    "\n",
    "\\begin{align*}\n",
    "&  \\min_{m,\\theta}m-\\hat{s}^{\\intercal}\\Phi\\theta\\\\\n",
    "s.t.~ &  m-\\left(  \\Phi\\theta\\right)  _{y}\\geq\\forall y\\in\\mathcal{Y}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-identification\n",
    "\n",
    "Note that the set of $\\theta$ that enter the solution to the problem above is not unique, but is a convex set. Denoting $V$ the value of program, we can look for bounds of $\\theta^{\\intercal}d$ for a chosen direction $d$ by\n",
    "\n",
    "\\begin{align*}\n",
    "& \\min_{\\theta,m}/\\max_{\\theta,m}   \\theta^{\\intercal}d\\\\\n",
    "s.t.~  &  m-\\hat{s}^{\\intercal}X\\theta=V\\\\\n",
    "&  m\\geq\\left(  \\Phi\\theta\\right)_{y}, \\quad \\forall y\\in\\mathcal{Y}%\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link with exponential families and GLM\n",
    "\n",
    "See class notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation with observed heterogeneity\n",
    "\n",
    "We now assume that we observe individual characteristics that are relevant for individual choices, that is $U_{iy}=\\sum_k \\Phi_{iyk} \\beta_k$, or in matrix form\n",
    "$$U = \\Phi \\beta,$$ where $\\beta\\in\\mathbb{R}^{p}$ is a parameter, and $\\Phi$ is a $\\left(\\left\\vert \\mathcal{I}\\left\\vert\\right\\vert\\mathcal{Y}\\right\\vert \\right) \\times p$ matrix.\n",
    "\n",
    "See class notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application\n",
    "\n",
    "Back to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi_iy_k = np.column_stack((np.kron(np.identity(4)[0:4,1:4],np.repeat(1, ninds).reshape(ninds,1)), - travelmode['travel'].values, - (travelmode['travel']*travelmode['income']).values, - travelmode['gcost'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbK = Phi_iy_k.shape[1]\n",
    "phi_mean = Phi_iy_k.mean(axis = 0)\n",
    "phi_stdev = Phi_iy_k.std(axis = 0, ddof = 1)\n",
    "Phi_iy_k = ((Phi_iy_k - phi_mean).T/phi_stdev[:,None]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(theta):\n",
    "    nbK = np.asarray(theta).shape[0]\n",
    "    Xtheta = Phi_iy_k.dot(theta)/sigma\n",
    "    Xthetamat_iy = Xtheta.reshape(nbchoices, ninds).T\n",
    "    max_i = np.amax(Xthetamat_iy, axis = 1)\n",
    "    expPhi_iy = np.exp((Xthetamat_iy.T -max_i).T)\n",
    "    d_i = np.sum(expPhi_iy, axis = 1)\n",
    "    \n",
    "    val = np.sum(np.multiply(Xtheta,muhat_iy))  - np.sum(max_i) - sigma * np.sum(np.log(d_i))\n",
    "\n",
    "    return -val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_log_likelihood(theta):\n",
    "    nbK = np.asarray(theta).shape[0]\n",
    "    Xtheta = Phi_iy_k.dot(theta)/sigma\n",
    "    Xthetamat_iy = Xtheta.reshape(nbchoices, ninds).T\n",
    "    max_i = np.amax(Xthetamat_iy, axis = 1)\n",
    "    expPhi_iy = np.exp((Xthetamat_iy.T -max_i).T)\n",
    "    d_i = np.sum(expPhi_iy, axis = 1)\n",
    "    \n",
    "    temp_mat = np.multiply(Phi_iy_k.T, expPhi_iy.T.flatten()).T\n",
    "    list_temp = []\n",
    "    for i in range(nbchoices):\n",
    "        list_temp.append(temp_mat[i*ninds:(i+1)*ninds,])\n",
    "    n_i_k = np.sum(list_temp,axis = 0)\n",
    "    \n",
    "    thegrad = muhat_iy.reshape(1,nbchoices*ninds).dot(Phi_iy_k).flatten() - np.sum(n_i_k.T/d_i, axis = 1)\n",
    "\n",
    "    return -thegrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = np.repeat(0,nbK)\n",
    "sigma = 1\n",
    "outcome = optimize.minimize(log_likelihood,method = 'CG',jac = grad_log_likelihood, x0 = theta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_mle = 1 / outcome['x'][nbK - 1]\n",
    "theta_mle = outcome['x']*temp_mle\n",
    "print(temp_mle)\n",
    "print(theta_mle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenobj = nbK+ninds\n",
    "c = np.concatenate((muhat_iy.reshape(1,nbchoices*ninds).dot(Phi_iy_k).flatten(),np.repeat(-1,ninds)))\n",
    "\n",
    "m = grb.Model('lp')\n",
    "x = m.addMVar(lenobj, name='x', lb=-grb.GRB.INFINITY)\n",
    "m.setObjective(c @ x, grb.GRB.MAXIMIZE)\n",
    "cstMat = spr.hstack((spr.csr_matrix(-Phi_iy_k), spr.kron(two_d(np.repeat(1,nbchoices)),spr.identity(ninds))))\n",
    "rhs = np.repeat(0,ninds*nbchoices)\n",
    "m.addConstr(cstMat @ x >= rhs)\n",
    "nbCstr = cstMat.shape[0]\n",
    "const_2 = np.array([0]*(nbK - 1))\n",
    "const_2 = np.append(const_2, 1)\n",
    "const_2 = np.append(const_2 ,[0]*ninds)\n",
    "m.addConstr(const_2 @ x == 1)\n",
    "m.optimize()\n",
    "if m.status == grb.GRB.Status.OPTIMAL:\n",
    "    print(\"Value of the problem (Gurobi) =\", m.objval)\n",
    "    opt_x = m.getAttr('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_lp = np.array(opt_x[:nbK])\n",
    "print(theta_lp)\n",
    "print(theta_mle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indMax=100\n",
    "tempMax=temp_mle\n",
    "outcomemat = np.zeros((indMax+1,nbK-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_fixedtemp(subsetoftheta, *temp):\n",
    "    val = log_likelihood(np.append(subsetoftheta, 1/temp[0]))\n",
    "    \n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_log_likelihood_fixedtemp(subsetoftheta, *temp):\n",
    "    val = grad_log_likelihood(np.append(subsetoftheta, 1/temp[0]))[:-1]\n",
    "    \n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomemat[0,:] = theta_lp[:-1]\n",
    "iterMax = indMax+1\n",
    "for k in range(2,iterMax+1,1):\n",
    "    thetemp = tempMax * (k-1)/indMax\n",
    "    outcomeFixedTemp = optimize.minimize(log_likelihood_fixedtemp,method = 'CG',jac = grad_log_likelihood_fixedtemp, args = (thetemp,),  x0 = theta0[:-1])\n",
    "    outcomemat[k-1,:] = outcomeFixedTemp['x']*thetemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomemat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zero-temperature estimator is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outcomemat[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mle estimator is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outcomemat[indMax,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbB = 100\n",
    "thetemp = 1\n",
    "epsilon_biy = special.digamma(1) -np.log(-np.log(np.random.uniform(0,1,ninds*nbchoices*nbB)))\n",
    "lenobj = ninds*nbB+nbK\n",
    "\n",
    "newc = np.concatenate((muhat_iy.reshape(1,nbchoices*ninds).dot(Phi_iy_k).flatten(),np.repeat(-1/nbB,ninds*nbB)))\n",
    "newm = grb.Model('new_lp')\n",
    "x = newm.addMVar(lenobj, name='x', lb=-grb.GRB.INFINITY)\n",
    "newm.setObjective(newc @ x, grb.GRB.MAXIMIZE)\n",
    "mat1 = spr.kron(-Phi_iy_k, two_d(np.repeat(1,nbB)))\n",
    "mat2 = spr.kron(two_d(np.repeat(1,nbchoices)),spr.identity(ninds*nbB))\n",
    "newcstMat = spr.hstack((mat1, mat2))\n",
    "rhs = epsilon_biy\n",
    "newm.addConstr(newcstMat @ x >= rhs)\n",
    "newm.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if m.status == grb.GRB.Status.OPTIMAL:\n",
    "    print(\"Value of the problem (Gurobi) =\", newm.objval)\n",
    "    opt_x = np.array(newm.getAttr('x'))\n",
    "newtheta_lp = opt_x[:nbK] / opt_x[nbK-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theta_mle)\n",
    "print(newtheta_lp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally probit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbB = 100\n",
    "thetemp = 1\n",
    "epsilon_biy = np.random.normal(nbB*ninds*nbchoices)\n",
    "lenobj = ninds*nbB+nbK\n",
    "\n",
    "newc = np.concatenate((muhat_iy.reshape(1,nbchoices*ninds).dot(Phi_iy_k).flatten(),np.repeat(-1/nbB,ninds*nbB)))\n",
    "newm = grb.Model('new_lp')\n",
    "x = newm.addMVar(lenobj, name='x', lb=-grb.GRB.INFINITY)\n",
    "newm.setObjective(newc @ x, grb.GRB.MAXIMIZE)\n",
    "mat1 = spr.kron(-Phi_iy_k, two_d(np.repeat(1,nbB)))\n",
    "mat2 = spr.kron(two_d(np.repeat(1,nbchoices)),spr.identity(ninds*nbB))\n",
    "newcstMat = spr.hstack((mat1, mat2))\n",
    "rhs = epsilon_biy\n",
    "newm.addConstr(newcstMat @ x >= rhs)\n",
    "newm.optimize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
